{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Sparse Vector to Embeddings with Encoder--Decoder Structure\n",
    "\n",
    "1. 求 Embeddings\n",
    "2. Encoder--Decoder 結構"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字典向量\n",
    "若我們字典裡有 $N$ 個 words, 第 $i$ 個字 $w^i$ 應該怎麼表示呢?\n",
    "\n",
    "通常使用 one-hot vector 來表示: 把$w^i$ 變成一個長度 $N$ 的向量 $x^i$。\n",
    "\n",
    "<img src=\"./pic/one-hot.png\" height=\"10%\" width=\"10%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "恭喜! 有了 vector 我們就可以套用數學模型了。\n",
    "\n",
    "問題是這樣的向量太稀疏了，尤其是當字典非常大的時候。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稀疏向量對於模型訓練很沒有效率。\n",
    "\n",
    "我們需要轉換到比較緊密的向量，通常稱為 embedding。\n",
    "\n",
    "下圖舉例將 $x$ 對應到它的緊密向量 $e$, 緊密向量有 `embed_dim` 維度\n",
    "\n",
    "<img src=\"./pic/one-hot-to-embed.png\" height=\"20%\" width=\"20%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先假設已知如何對應到緊密向量\n",
    "\n",
    "已知一個 $N * embed\\_dim$ 的矩陣 $E$，第 $i$ 個 row $e^i$ 就是 $w^i$ 的 embedding。\n",
    "\n",
    "<img src=\"./pic/embedMat.png\" height=\"20%\" width=\"20%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們就可以使用 $e$ 來代替原先的稀疏向量 $x$ 進行訓練，讓訓練更好更容易。\n",
    "\n",
    "以一個語言模型來說，使用 LSTM 模型如下:\n",
    "\n",
    "<img src=\"./pic/lm1.png\" height=\"60%\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恩，這樣大功告成，我們的模型可以順利訓練 .... ??\n",
    "\n",
    "不對，$E$ 這個 lookup table 怎麼決定?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Table 使用矩陣相乘\n",
    "\n",
    "答案是讓模型**自己訓練決定**。要更了解內部運作，我們先將 lookup table 使用矩陣相乘的方式來看。\n",
    "\n",
    "<img src=\"./pic/matmul4lookup.png\" height=\"70%\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以使用 lookup table LSTM 的語言模型變成如下\n",
    "<img src=\"./pic/lm2.png\" height=\"50%\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "等等，矩陣相乘不就跟 neural net 一樣嗎?\n",
    "\n",
    "這樣看起來這個 lookup table $E$ 就是一層的類神經網路而已 (沒有 activation function)。\n",
    "\n",
    "我們用 LL (Linear Layer) 來代表，$E$ 就是 LL 的 weight matrix。\n",
    "\n",
    "<img src=\"./pic/lm3.png\" height=\"50%\" width=\"50%\"/>\n",
    "\n",
    "表示成 neural net 的方式，我們就直接可以 Backprob 訓練出 LL 的 weight $E$ 了。而 $E$ 就是我們要找的 embeddings。\n",
    "\n",
    "> 1. Tensorflow 中做這樣的 lookup table 可以使用 [tf.nn.embedding_lookup()](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup)。\n",
    "> 2. Embedding 的作法可參考 tf [官網此處](https://www.tensorflow.org/programmers_guide/embedding)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LL很弱怎麼辦?\n",
    "\n",
    "只用**一層**線性組合 (LL) 就想把特徵擷取做到很好，似乎有點簡化了。\n",
    "\n",
    "沒錯，我們都知道，特徵擷取是 **Deep** neural net 的拿手好戲，所以我們可以將 LL 換成強大的 CNN。\n",
    "\n",
    "<img src=\"./pic/lm4.png\" height=\"50%\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這種先經過一層特徵擷取，再做辨識，其實跟 Encoder -- Decoder 的架構一樣。\n",
    "\n",
    "<img src=\"./pic/lm5.png\" height=\"50%\" width=\"50%\"/>\n",
    "\n",
    "都是先經過 Encoder 做出 embeddings，接著使用 Embeddings decode 出結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder 如果也採用 RNN 的話基本上就是 [sequence-to-sequence](http://arxiv.org/abs/1409.3215) 的架構了。\n",
    "\n",
    "<img src=\"./pic/encoder-decoder.png\" height=\"100%\" width=\"100%\"/>\n",
    "\n",
    "基本上拓展一下，對圖或影像做 Encode，而 Decoder 負責解碼出描述的文字。或是語言翻譯，語音辨識，都可以這麼看待。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. Embedding tf 官網 [link](https://www.tensorflow.org/programmers_guide/embedding)\n",
    "2. Sequence to sequence learning [link](http://arxiv.org/abs/1409.3215)\n",
    "3. Udacity lstm [github](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb)\n",
    "4. [colah lstm](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
